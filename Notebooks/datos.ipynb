{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todas las librerias necesarias\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://agroambient.gva.es/es/precios-agrarios' #Escogemos la url de la pagina web de la Conselleria de Agricultura\n",
    "\n",
    "response = requests.get(url) # Hacemos una request con esa url a traves de la libreria requests\n",
    "\n",
    "print(response) # Printeamos para ver si ha sido posible la conexión\n",
    "\n",
    "html = response.content # Obtenemos la informacion html de la web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(html, \"lxml\") # abrimos el contenido html con la libreria Beautiful Soup\n",
    "\n",
    "li_tags = list(soup.select(\"li.file a\")) # de la pagina html solamente nos interesan los archivos 'a' contenidos dentro de la clase file del archivo 'li'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_excel = [] # Cremaos lista de excel vacia\n",
    "\n",
    "for i in range(len(li_tags)): #iteramos a traves de la variable li_tags \n",
    "\n",
    "    if li_tags[i].get(\"title\", \"holixls\").endswith('xls'): # Nos quedamos únicamente con los archivos acabados en .xls, osea los excel.\n",
    "        enlaces = li_tags[i]['href'] # de estos archivos nos quedemos con los enlaces a las hojas excel\n",
    "        lista_excel.append(enlaces) # y las añadimos a la lista excel vacia\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "excel_df = pd.DataFrame(lista_excel, columns = [\"media_url\"]) # Creamos un dataframe con la lista excel y una columna llamada 'media url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_df[\"url\"]= \"https://agroambient.gva.es\" + excel_df.media_url[~excel_df.media_url.str.startswith(\"https\")] # añadimos el resto de la url a las obtenidas anteriormente\n",
    "\n",
    "lista_url_completas = [i for i in excel_df['url'].dropna()] # Hacemos una lista con las url completas y eliminamos los nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cremaos listas vacias equivalentes a los años 2016 hasta el 2022\n",
    "aniodelsenior2022 = []\n",
    "aniodelsenior2021 = []\n",
    "aniodelsenior2020 = []\n",
    "aniodelsenior2019 = []\n",
    "aniodelsenior2018 = []\n",
    "aniodelsenior2017 = []\n",
    "aniodelsenior2016 = []\n",
    "\n",
    "for i in range(len(lista_url_completas)): # iteramos sobre la lista de urls completas y añadimos la url de un año determinado a su lista correspondiente creada anteriormente\n",
    "    if '2022' in lista_url_completas[i]:\n",
    "        aniodelsenior2022.append(lista_url_completas[i])\n",
    "    elif '2021' in lista_url_completas[i]:\n",
    "        aniodelsenior2021.append(lista_url_completas[i])\n",
    "    elif '2020' in lista_url_completas[i]:\n",
    "        aniodelsenior2020.append(lista_url_completas[i])\n",
    "    elif '2019' in lista_url_completas[i]:\n",
    "        aniodelsenior2019.append(lista_url_completas[i])\n",
    "    elif '2018' in lista_url_completas[i]:\n",
    "        aniodelsenior2018.append(lista_url_completas[i])\n",
    "    elif '2017' in lista_url_completas[i]:\n",
    "        aniodelsenior2017.append(lista_url_completas[i])\n",
    "    elif '2016' in lista_url_completas[i]:\n",
    "        aniodelsenior2016.append(lista_url_completas[i])\n",
    "    \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos dataframes vacios para añadir el contenido de las urls\n",
    "\n",
    "df_2022 = pd.DataFrame()\n",
    "df_2021 = pd.DataFrame()\n",
    "df_2020 = pd.DataFrame()\n",
    "df_2019 = pd.DataFrame()\n",
    "df_2018 = pd.DataFrame()\n",
    "df_2017 = pd.DataFrame()\n",
    "df_2016 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner (aniodelsenior,df): #funcion para limpiar los datasets \n",
    "    \n",
    "    for f in aniodelsenior:\n",
    "        print(f) # printeamos la url para comprobar que la función esta trabajando\n",
    "        data = pd.read_excel(f) #leemos el excel a través de la url \n",
    "        data1 = data.dropna(how='all').copy() # Eliminamos aquellas columnas que contienen todo nan\n",
    "        col_data = list(data.columns) # creamos una lista con las columnas del dataframe\n",
    "        data2 = data1.loc[data1[col_data[0]] != 'PRODUCTO'] # eliminamos de los datos aquellas columnas que se utilizan como header\n",
    "        data2[[col_data[0], col_data[1], col_data[2]]] = data2[[col_data[0], col_data[1], col_data[2]]].fillna(method='ffill') # Cambiamos los nan por el valor situado inmediatamente por encima\n",
    "        data1_drop = data2.dropna(thresh=4).loc[:,:col_data[6]] # Eliminamos las filas con menos de 4 valores no nan\n",
    "        data1_drop2 = data1_drop.loc[:,[col_data[0], col_data[1], col_data[2], col_data[3], col_data[4], col_data[6]]] #cogemos solo las columnas del 0 al 4 y la 6\n",
    "        data1_drop3 = data1_drop2.rename(columns={col_data[0]: 'producto', col_data[1]:'subtipo', col_data[2]: 'tipo_comercial', col_data[3]:'mercado', col_data[4]:'precio_max', col_data[6]:'precio_min'}) # renombramos columnas\n",
    "        if 'enero' in f: # para cada url correspondiente a un mes en concreto\n",
    "            data1_drop3['mes'] = 'enero' # creamos una columna con el nombre de ese mes\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0] # Y otra columna con el nombre de esa semana\n",
    "            df= df.append(data1_drop3) # Añadimos el dataframe al dataframe vacio creado en la celda anterior\n",
    "        elif 'febrero' in f:\n",
    "            data1_drop3['mes'] = 'febrero'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'marzo' in f:\n",
    "            data1_drop3['mes'] = 'marzo'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'abril' in f:\n",
    "            data1_drop3['mes'] = 'abril'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'mayo' in f:\n",
    "            data1_drop3['mes'] = 'mayo'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'junio' in f:\n",
    "            data1_drop3['mes'] = 'junio'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'julio' in f:\n",
    "            data1_drop3['mes'] = 'julio'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'agosto' in f:\n",
    "            data1_drop3['mes'] = 'agosto'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'septiembre' in f:\n",
    "            data1_drop3['mes'] = 'septiembre'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'octubre' in f:\n",
    "            data1_drop3['mes'] = 'octubre'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'noviembre' in f:\n",
    "            data1_drop3['mes'] = 'noviembre'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'diciembre' in f:\n",
    "            data1_drop3['mes'] = 'diciembre'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return df # la función devuelve el dataframe creado en la celda anterior con toda la información extraida de las url de cada mismo año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner2 (aniodelsenior,df): # Esta función es muy parecida a la anterior solo cambia una cosa\n",
    "    \n",
    "    for f in aniodelsenior:\n",
    "        print(f)\n",
    "        data = pd.read_excel(f)\n",
    "        data1 = data.dropna(how='all').copy()\n",
    "        col_data = list(data.columns)\n",
    "        data1[[col_data[0], col_data[1], col_data[2]]] = data1[[col_data[0], col_data[1], col_data[2]]].fillna(method='ffill')\n",
    "        data1_drop = data1.dropna(thresh=4).loc[:,:col_data[4]]\n",
    "        data1_drop2 = data1_drop.loc[:,[col_data[0], col_data[1], col_data[2], col_data[3], col_data[4]]]\n",
    "        data1_drop3 = data1_drop2.rename(columns={col_data[0]: 'producto', col_data[1]:'subtipo', col_data[2]: 'tipo_comercial', col_data[3]:'mercado', col_data[4]:'precio'}) # aqui no hay precio maximo y minimo, ambos estan incluidos en una misma columna con el siguiente formato: preciomin/preciomax\n",
    "        if 'enero' in f:\n",
    "            data1_drop3['mes'] = 'enero'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df= df.append(data1_drop3)    \n",
    "        elif 'febrero' in f:\n",
    "            data1_drop3['mes'] = 'febrero'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'marzo' in f:\n",
    "            data1_drop3['mes'] = 'marzo'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'abril' in f:\n",
    "            data1_drop3['mes'] = 'abril'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'mayo' in f:\n",
    "            data1_drop3['mes'] = 'mayo'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'junio' in f:\n",
    "            data1_drop3['mes'] = 'junio'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'julio' in f:\n",
    "            data1_drop3['mes'] = 'julio'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'agosto' in f:\n",
    "            data1_drop3['mes'] = 'agosto'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'septiembre' in f:\n",
    "            data1_drop3['mes'] = 'septiembre'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'octubre' in f:\n",
    "            data1_drop3['mes'] = 'octubre'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'noviembre' in f:\n",
    "            data1_drop3['mes'] = 'noviembre'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        elif 'diciembre' in f:\n",
    "            data1_drop3['mes'] = 'diciembre'\n",
    "            data1_drop3['semana'] = re.findall('Semana\\+\\d\\d',f)[0]\n",
    "            df = df.append(data1_drop3)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df22 = cleaner(aniodelsenior2022, df_2022) #usamos la funcion con la lista de urls del año correspondiente y el dataframe vacio de ese mismo año\n",
    "df21 = cleaner(aniodelsenior2021, df_2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aniodelsenior2020.remove('https://agroambient.gva.es/documents/162218839/169358588/Semana+49%2C+del+1+al+7+de+diciembre+de+2020+xls.pdf/f95ca99e-a1fd-449a-ad15-90484d7e63cd?t=1608124157193') # eliminamos este archivo que en realidad es un pdf\n",
    "\n",
    "df20 = cleaner(aniodelsenior2020,df_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aniodelsenior2019.remove('https://agroambient.gva.es/documents/162218839/167325931/Semana+46%2C+del+12+al+18+de+noviembre+de+2019+xls.xls/ac9fbfc9-31b9-42ff-b86e-e439711e29ad?t=1574250433611') # el archivo excel no funciona\n",
    "\n",
    "df19 = cleaner(aniodelsenior2019,df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aniodelsenior2018_1 = aniodelsenior2018[0:29] # El año 2018 tiene los precios dividos en dos columnas precio max y min pero a partir de la fila 29 los precios maximo y minimo se ponen en la misma columna\n",
    "df18_1 = cleaner(aniodelsenior2018_1, df_2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aniodelsenior2018_2 = aniodelsenior2018[29:]\n",
    "df18_2 = cleaner2(aniodelsenior2018_2, df_2018) #Aquí usamos la segunda función de limpieza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df17 = cleaner2(aniodelsenior2017, df_2017)\n",
    "df16 = cleaner2(aniodelsenior2016, df_2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de Superficie del REGEPA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos primero cual es la cantidad de naranjas que se plantan en Valencia en relación con otros frutales\n",
    "\n",
    "regepa17 = pd.read_excel('regepa2017.xls', header=2)\n",
    "\n",
    "regepa18 = pd.read_excel('regepa2018.xlsx', header=2)\n",
    "\n",
    "regepa19 = pd.read_excel('regepa2019.xlsx', header=2)\n",
    "\n",
    "regepa20 = pd.read_excel('regepa2020.xlsx', header=2)\n",
    "\n",
    "regepa17['año'] = 2017\n",
    "regepa18['año'] = 2018\n",
    "regepa19['año'] = 2019\n",
    "regepa20['año'] = 2020\n",
    "\n",
    "regepa17 = regepa17.rename(columns={'CULTIVO ':'CULTIVO'})\n",
    "regepa18 = regepa18.rename(columns={'CULTIVO ':'CULTIVO'})\n",
    "regepa19 = regepa19.rename(columns={'CULTIVO ':'CULTIVO'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "regepa_total = pd.concat([regepa17,regepa18,regepa19,regepa20]) # Juntamos los datos de todos los años\n",
    "regepa_total_valencia = regepa_total.loc[regepa_total['PROVINCIA'] == 'VALENCIA', :] # Seleccionamos los de la provincia de Valencia\n",
    "regepa_frutas_citricos_valencia = regepa_total_valencia.loc[(regepa_total_valencia['GRUPO DE CULTIVO'] == 'CITRICOS') | (regepa_total_valencia['GRUPO DE CULTIVO'] == 'FRUTALES'),:] # Seleccionamos los frutales y los citricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto lo guardamos como csv en nuestra carpeta\n",
    "regepa_frutas_citricos_valencia.to_csv(r'C:\\Users\\Usuario\\Desktop\\EDA KAQUI\\regepa_total4.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8fce248eab8eab509e0740a3840010fbf0ba565be458b8a69805e9003fe762b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('EDA_environment')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
